{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 12:41:43.543175: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-06 12:41:43.565262: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-06 12:41:43.906592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 12:41:45.971029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 12:41:45.990266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 12:41:45.994855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Check if GPU is available\n",
    "if tf.config.experimental.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU not found, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Text8: 4300 files\n",
      "Class Text5: 4300 files\n",
      "Class Text6: 4300 files\n",
      "Class Text7: 4300 files\n",
      "Class Text4: 4300 files\n",
      "Class Text3: 2380 files\n",
      "X shape: (23880, 300, 8)\n",
      "y shape: (23880,)\n"
     ]
    }
   ],
   "source": [
    "data_path = r'/home/fede/PhD/UNIGE/Dataset_UniCa/Cagliari/Dataset'\n",
    "\n",
    "Glob_data = glob.glob(data_path + \"/*/*\")\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, base_file_path):\n",
    "        # Initialize with a base file path, set a maximum number of files per class, and initialize variables for data\n",
    "        self.max_files_per_class = 50\n",
    "        self.base_file_path = base_file_path\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.class_counts = {}\n",
    "        self.text_files = self._get_text_files()\n",
    "        \n",
    "    def _get_text_files(self):\n",
    "        # Get all directories in the base file path\n",
    "        data = glob.glob(self.base_file_path + \"/*\")\n",
    "        category_names = [os.path.basename(directory_path) for directory_path in data]\n",
    "        text_files = {category: [] for category in category_names}\n",
    "        \n",
    "        # Map each category to its corresponding directories\n",
    "        for directory_path in data:\n",
    "            category = os.path.basename(directory_path)\n",
    "            if category in text_files:\n",
    "                text_files[category].append(directory_path)\n",
    "        \n",
    "        return text_files\n",
    "    \n",
    "    def _load_data(self):\n",
    "        # Load data from each file into X and y arrays\n",
    "        for label, (key, directory_paths) in enumerate(self.text_files.items(), start=0):\n",
    "            self.class_counts[key] = 0\n",
    "            for directory_path in directory_paths:\n",
    "                files_in_directory = glob.glob(directory_path + \"/*.txt\")  # Looking for .txt files specifically\n",
    "                for file_path in files_in_directory:\n",
    "                    df = pd.read_csv(file_path, header=None,delimiter='\\t', skiprows=1)\n",
    "                    \n",
    "                    df= df.iloc[4000:-4000, :]\n",
    "                      # Extract specific part of the dataframe\n",
    "                    scaler = StandardScaler()\n",
    "                    df_normalized = scaler.fit_transform(df)\n",
    "                    \n",
    "                    # Randomly select three windows of 300 samples\n",
    "                    for _ in range(10):\n",
    "                        start_idx = random.randint(0, df_normalized.shape[0] - 300)\n",
    "                        df_extract = df_normalized[start_idx:start_idx + 300, :]\n",
    "                        # Append to the corresponding list based on the Text category\n",
    "                        self.X.append(df_extract)\n",
    "                        self.y.append(int(label))  \n",
    "                        self.class_counts[key] += 1  # Update class count  \n",
    "\n",
    "                  \n",
    "                print(f\"Class {key}: {self.class_counts[key]} files\")\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        # Convert X and y to numpy arrays and ensure correct data types\n",
    "        self.X = np.asarray(self.X, dtype=np.float32)\n",
    "        self.y = np.asarray(self.y, dtype=np.int32)\n",
    "        print(\"X shape:\", self.X.shape)\n",
    "        print(\"y shape:\", self.y.shape)\n",
    "        \n",
    "    def get_data_splits(self):\n",
    "        # Load data, prepare it, and split it into training, validation, and test sets\n",
    "        self._load_data()\n",
    "        self._prepare_data()\n",
    "        y_categorical = tf.keras.utils.to_categorical(self.y)\n",
    "        X_train, x_tmp, y_train, y_tmp = train_test_split(self.X, y_categorical, test_size=0.3, random_state=42, stratify=self.y)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(x_tmp, y_tmp, test_size=0.5, random_state=42, stratify=np.argmax(y_tmp, axis=1))\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    \n",
    "X_train, y_train, X_val, y_val, X_test, y_test = Dataset(data_path).get_data_splits()\n",
    "\n",
    "with open ('dataset.npy','wb') as f:\n",
    "    np.save(f, X_train)\n",
    "    np.save(f, y_train)\n",
    "    np.save(f, X_val)\n",
    "    np.save(f, y_val)\n",
    "    np.save(f, X_test)\n",
    "    np.save(f, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Text8: 4300 files\n",
      "Class Text5: 4300 files\n",
      "Class Text6: 4300 files\n",
      "Class Text7: 4300 files\n",
      "Class Text4: 4300 files\n",
      "Class Text3: 2380 files\n",
      "X shape: (23880, 300, 8)\n",
      "y shape: (23880,)\n",
      "X_train shape: (16716, 300, 8)\n",
      "y_train shape: (16716, 8)\n",
      "X_val shape: (3582, 300, 8)\n",
      "y_val shape: (3582, 8)\n",
      "X_test shape: (3582, 300, 8)\n",
      "y_test shape: (3582, 8)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, base_file_path):\n",
    "        # Initialize with a base file path, set a maximum number of files per class, and initialize variables for data\n",
    "        self.max_files_per_class = 50\n",
    "        self.base_file_path = base_file_path\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.class_counts = {}\n",
    "        self.text_files = self._get_text_files()\n",
    "        \n",
    "    def _get_text_files(self):\n",
    "        # Get all directories in the base file path\n",
    "        data = glob.glob(self.base_file_path + \"/*\")\n",
    "        category_names = [os.path.basename(directory_path) for directory_path in data]\n",
    "        text_files = {category: [] for category in category_names}\n",
    "        \n",
    "        # Map each category to its corresponding directories\n",
    "        for directory_path in data:\n",
    "            category = os.path.basename(directory_path)\n",
    "            if category in text_files:\n",
    "                text_files[category].append(directory_path)\n",
    "        \n",
    "        return text_files\n",
    "    \n",
    "    def _load_data(self):\n",
    "        # Load data from each file into X and y arrays\n",
    "        for label, (key, directory_paths) in enumerate(self.text_files.items(), start=0):\n",
    "            self.class_counts[key] = 0\n",
    "            for directory_path in directory_paths:\n",
    "                files_in_directory = glob.glob(directory_path + \"/*.txt\")  # Looking for .txt files specifically\n",
    "                for file_path in files_in_directory:\n",
    "                    df = pd.read_csv(file_path, header=None, delimiter='\\t', skiprows=1)\n",
    "                    \n",
    "                    df = df.iloc[4000:-4000, :]  # Extract specific part of the dataframe\n",
    "                    scaler = StandardScaler()\n",
    "                    df_normalized = scaler.fit_transform(df)\n",
    "                    \n",
    "                    # Randomly select three windows of 300 samples\n",
    "                    for _ in range(10):\n",
    "                        start_idx = random.randint(0, df_normalized.shape[0] - 300)\n",
    "                        df_extract = df_normalized[start_idx:start_idx + 300, :]\n",
    "                        # Append to the corresponding list based on the Text category\n",
    "                        self.X.append(df_extract)\n",
    "                        self.y.append(int(label))  \n",
    "                        self.class_counts[key] += 1  # Update class count  \n",
    "\n",
    "                print(f\"Class {key}: {self.class_counts[key]} files\")\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        # Convert X and y to numpy arrays and ensure correct data types\n",
    "        self.X = np.asarray(self.X, dtype=np.float32)\n",
    "        self.y = np.asarray(self.y, dtype=np.int32)\n",
    "        print(\"X shape:\", self.X.shape)\n",
    "        print(\"y shape:\", self.y.shape)\n",
    "        \n",
    "    def get_data_splits(self):\n",
    "        # Load data, prepare it, and split it into training, validation, and test sets\n",
    "        self._load_data()\n",
    "        self._prepare_data()\n",
    "        \n",
    "        # Convert y labels to one-hot encoding\n",
    "        y_categorical = tf.keras.utils.to_categorical(self.y, num_classes=8)  # One-hot encoding\n",
    "        \n",
    "        # Split the dataset\n",
    "        X_train, x_tmp, y_train, y_tmp = train_test_split(self.X, y_categorical, test_size=0.3, random_state=42, stratify=self.y)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(x_tmp, y_tmp, test_size=0.5, random_state=42, stratify=np.argmax(y_tmp, axis=1))\n",
    "        \n",
    "        # Return the training, validation, and test splits\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    \n",
    "# Initialize and process the dataset\n",
    "data_path = r'/home/fede/PhD/UNIGE/Dataset_UniCa/Cagliari/Dataset'\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = Dataset(data_path).get_data_splits()\n",
    "\n",
    "# Save the processed dataset to file\n",
    "with open('dataset.npy', 'wb') as f:\n",
    "    np.save(f, X_train)\n",
    "    np.save(f, y_train)\n",
    "    np.save(f, X_val)\n",
    "    np.save(f, y_val)\n",
    "    np.save(f, X_test)\n",
    "    np.save(f, y_test)\n",
    "\n",
    "# Optional: Print the shapes of the data to verify\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
