{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for MDC tool\n",
    "\n",
    "In this notebook, it is presented a brief tutorial on how to define and train a small Convolutional Neural Network for the classification of the MNIST Dataset. At the end of the notebook, it will be showed how to convert the keras model into the QONNX format.\n",
    "\n",
    "The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image (https://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "![alt text](images/mnist_eg.png \"MNIST example\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras (https://keras.io/) is an open source free library that gives access to an interface for Neural Networks (NN) in Python. It is now integrated into the Tensorflow library.\n",
    "With Keras we have the possibility of defining and training neural networks. QKeras (https://github.com/google/qkeras) is a quantization extension to Keras that provides drop-in replacement for some of the Keras layers, especially the ones that creates parameters and activation layers, and perform arithmetic operations, so that we can quickly create a deep quantized version of Keras network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are going to explore the capabilities of Qkeras, by defining and training a Convolutional Neural Network.\n",
    "First, we import the necessaries packages and do some checks on libraries versions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if TensorFlow is using the GPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a folder to store the outputs of this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/fede/PhD/qonnx2mdc\n",
      "Folder 'Mnist_Training' already exists.\n",
      "/home/fede/PhD/qonnx2mdc/Mnist_Training\n"
     ]
    }
   ],
   "source": [
    "# Specify the folder name\n",
    "folder_name = 'Mnist_Training'\n",
    "\n",
    "script_path = os.getcwd()\n",
    "# Get the current working directory\n",
    "current_directory = os.path.dirname(script_path)\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory:\", current_directory)\n",
    "\n",
    "\n",
    "# Create the full path to the new folder\n",
    "output_path = current_directory + \"/\" + folder_name\n",
    "\n",
    "# Check if the folder already exists\n",
    "if not os.path.exists(output_path):\n",
    "    # Create the folder\n",
    "    os.makedirs(output_path)\n",
    "    print(f\"Folder '{folder_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists.\")\n",
    "\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to load the MNIST dataset, and to extract information like training size (train_size), the input shape (input__shape) and the number of classes to classify (n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 60000 samples of input shape (28, 28, 1), belonging to 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 15:27:53.519447: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_train, info = tfds.load('mnist', split='train[:90%]', with_info=True, as_supervised=True)\n",
    "ds_test = tfds.load('mnist', split='test', shuffle_files=True, as_supervised=True)\n",
    "ds_val = tfds.load('mnist', split='train[-10%:]', shuffle_files=True, as_supervised=True)\n",
    "\n",
    "assert isinstance(ds_train, tf.data.Dataset)\n",
    "train_size = int(info.splits['train'].num_examples)\n",
    "input_shape = info.features['image'].shape\n",
    "n_classes = info.features['label'].num_classes\n",
    "\n",
    "print('Training on {} samples of input shape {}, belonging to {} classes'.format(train_size, input_shape, n_classes))\n",
    "fig = tfds.show_examples(ds_train, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to apply some preprocessing to the dataset and we manage the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label, nclasses=10):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    label = tf.one_hot(tf.squeeze(label), nclasses)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train batch shape = (1024, 28, 28, 1), Y train batch shape = (1024, 10) \n",
      "X test batch shape = (10000, 28, 28, 1), Y test batch shape = (10000, 10) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 10:36:00.875821: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train_data = ds_train.map(preprocess, n_classes)  # Get dataset as image and one-hot encoded labels, divided by max RGB\n",
    "train_data = train_data.shuffle(4096).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "for example in train_data.take(1):\n",
    "    break\n",
    "print(\"X train batch shape = {}, Y train batch shape = {} \".format(example[0].shape, example[1].shape))\n",
    "\n",
    "val_data = ds_val.map(preprocess, n_classes)\n",
    "val_data = val_data.batch(batch_size)\n",
    "val_data = val_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# For  testing, we get the full dataset in memory as it's rather small.\n",
    "# We fetch it as numpy arrays to have access to labels and images separately\n",
    "X_test, Y_test = tfds.as_numpy(tfds.load('mnist', split='test', batch_size=-1, as_supervised=True))\n",
    "X_test, Y_test = preprocess(X_test, Y_test, nclasses=n_classes)\n",
    "print(\"X test batch shape = {}, Y test batch shape = {} \".format(X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model is defined. Each layer has its own kernel_quantizers and bias_quantizers, which are functions that apply the quantization to, respectively, weights and biases of the Conv and Gemm layers, and QActivations, which quantizes the activations. While all the Relu layers are quantized, even if with different precisions for each model, we always left untouched the last activation, the Sigmoid layer, to achieve a better accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"qkeras\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " q_conv2d (QConv2D)          (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " act_1 (QActivation)         (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " max_pool_1 (MaxPooling2D)   (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " q_conv2d_1 (QConv2D)        (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " act_2 (QActivation)         (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " max_pool_2 (MaxPooling2D)   (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1568)              0         \n",
      "                                                                 \n",
      " q_dense (QDense)            (None, 10)                15690     \n",
      "                                                                 \n",
      " output_sigmoid (Activation  (None, 10)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25258 (98.66 KB)\n",
      "Trainable params: 25258 (98.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fede/miniconda3/envs/gpu_env/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer HeNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from qkeras import *\n",
    "from qkeras.qlayers import QDense, QActivation, quantized_bits, quantized_relu\n",
    "from qkeras import QConv2D\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "\n",
    "x = x_in = Input(shape=input_shape, name = \"input_layer\")\n",
    "\n",
    "x = QConv2D(32, (3,3), name = \"q_conv2d\", padding='same',kernel_quantizer= quantized_bits(bits=8, integer=4, alpha=1),\n",
    "    bias_quantizer= quantized_bits(bits=8, integer=4, alpha=1))(x)\n",
    "x = QActivation(quantized_relu(bits=16, integer=8, use_sigmoid=0, negative_slope=0.0), name=\"act_1\")(x)\n",
    "x = MaxPooling2D(pool_size=(2,2),name = \"max_pool_1\")(x)\n",
    "x = QConv2D(32, (3,3), name = \"q_conv2d_1\",padding='same',kernel_quantizer= quantized_bits(bits=4, integer=2, alpha=1),\n",
    "    bias_quantizer= quantized_bits(bits=4, integer=2, alpha=1))(x)\n",
    "x = QActivation(quantized_relu(bits=16, integer=8, use_sigmoid=0, negative_slope=0.0), name=\"act_2\")(x)\n",
    "x = MaxPooling2D(pool_size=(2,2), name = \"max_pool_2\")(x)\n",
    "x = Flatten(name = \"flatten\")(x)\n",
    "x = QDense((10),name = \"q_dense\",kernel_quantizer= quantized_bits(bits=8, integer=4, alpha=1),\n",
    "    bias_quantizer= quantized_bits(bits=8, integer=4, alpha=1)) (x)   # num_classes = 10\n",
    "x_out = Activation('sigmoid', name='output_sigmoid')(x)\n",
    "\n",
    "qmodel = Model(inputs=[x_in], outputs=[x_out], name='qkeras')\n",
    "\n",
    "qmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the training phase can start. A low number of epochs is chosen as the model is fairly small and simple, leading to a low training time. We save both the whole model and separately, its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "\n",
    "n_epochs = 5\n",
    "if train:\n",
    "    LOSS = tf.keras.losses.CategoricalCrossentropy()\n",
    "    OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=3e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "    qmodel.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
    "    ]\n",
    "\n",
    "    start = time.time()\n",
    "    history = qmodel.fit(train_data, epochs=n_epochs, validation_data=val_data, callbacks=callbacks, verbose=1)\n",
    "    end = time.time()\n",
    "    print('\\n It took {} minutes to train!\\n'.format((end - start) / 60.0))\n",
    "\n",
    "    qmodel.save_weights(\"model_weights.h5\")\n",
    "\n",
    "    qmodel.save('model_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the keras model can be converted into the QONNX format. The QONNX format is an exstension of ONNX, an open format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers (https://onnx.ai/).\n",
    "\n",
    "QONNX (Quantized ONNX), starting from ONNX, introduces three new custom operators, Quant, BipolarQuant, and Trunc, in order to represent arbitrary-precision uniform quantization in ONNX. This enables representation of binary, ternary, 3-bit, 4-bit, 6-bit or any other quantization (https://github.com/fastmachinelearning/qonnx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.converters.keras import from_keras\n",
    "\n",
    "path = output_path + '/qonnx_model_1.onnx'\n",
    "print(\"conversion to qonnx...\")\n",
    "qonnx_model, _  = from_keras(\n",
    "    qmodel,\n",
    "    name=\"qkeras_to_qonnx_converted\",\n",
    "    input_signature=None,\n",
    "    opset=None,\n",
    "    custom_ops=None,\n",
    "    custom_op_handlers=None,\n",
    "    custom_rewriter=None,\n",
    "    inputs_as_nchw=None,\n",
    "    extra_opset=None,\n",
    "    shape_override=None,\n",
    "    target=None,\n",
    "    large_model=False,\n",
    "    output_path = path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"qkeras\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " q_conv2d1 (QConv2D)         (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 28, 28, 32)        128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " act_1 (QActivation)         (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " q_conv2d2 (QConv2D)         (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 28, 28, 32)        128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " act_2 (QActivation)         (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " max_pool_1 (MaxPooling2D)   (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " q_conv2d3 (QConv2D)         (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 14, 14, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " act_3 (QActivation)         (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " q_conv2d4 (QConv2D)         (None, 14, 14, 32)        18464     \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 14, 14, 32)        128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " act_4 (QActivation)         (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " max_pool_2 (MaxPooling2D)   (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1568)              0         \n",
      "                                                                 \n",
      " dense1 (Dense)              (None, 512)               803328    \n",
      "                                                                 \n",
      " dense2 (Dense)              (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 855626 (3.26 MB)\n",
      "Trainable params: 855306 (3.26 MB)\n",
      "Non-trainable params: 320 (1.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, BatchNormalization, MaxPooling2D, Dense, Flatten\n",
    "from qkeras import QConv2D, QActivation, quantized_bits\n",
    "\n",
    "input_shape = (32,32,3)\n",
    "\n",
    "x = x_in = Input(shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "# First Convolutional Block\n",
    "x = QConv2D(32, (3,3), name=\"q_conv2d1\", padding='same',\n",
    "    kernel_quantizer=quantized_bits(bits=8, integer=4, alpha=1),\n",
    "    bias_quantizer=quantized_bits(bits=8, integer=4, alpha=1))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation(quantized_relu(bits=16, integer=8, use_sigmoid=0, negative_slope=0.0), name=\"act_1\")(x)\n",
    "\n",
    "# Second Convolutional Block\n",
    "x = QConv2D(32, (3,3), name=\"q_conv2d2\", padding='same',\n",
    "    kernel_quantizer=quantized_bits(bits=8, integer=4, alpha=1),\n",
    "    bias_quantizer=quantized_bits(bits=8, integer=4, alpha=1))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation(quantized_relu(bits=16, integer=8, use_sigmoid=0, negative_slope=0.0), name=\"act_2\")(x)\n",
    "\n",
    "# First MaxPooling Layer\n",
    "x = MaxPooling2D(pool_size=(2,2), name=\"max_pool_1\")(x)\n",
    "\n",
    "# Third Convolutional Block (with 64 filters)\n",
    "x = QConv2D(64, (3,3), name=\"q_conv2d3\", padding='same',\n",
    "    kernel_quantizer=quantized_bits(bits=8, integer=4, alpha=1),\n",
    "    bias_quantizer=quantized_bits(bits=4, integer=2, alpha=1))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation(quantized_relu(bits=16, integer=8, use_sigmoid=0, negative_slope=0.0), name=\"act_3\")(x)\n",
    "\n",
    "# Fourth Convolutional Block (with 32 filters)\n",
    "x = QConv2D(32, (3,3), name=\"q_conv2d4\", padding='same',\n",
    "    kernel_quantizer=quantized_bits(bits=8, integer=4, alpha=1),\n",
    "    bias_quantizer=quantized_bits(bits=4, integer=2, alpha=1))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation(quantized_relu(bits=16, integer=8, use_sigmoid=0, negative_slope=0.0), name=\"act_4\")(x)\n",
    "\n",
    "# Second MaxPooling Layer\n",
    "x = MaxPooling2D(pool_size=(2,2), name=\"max_pool_2\")(x)\n",
    "\n",
    "# Flatten the output before Dense layers\n",
    "x = Flatten(name=\"flatten\")(x)\n",
    "\n",
    "# Fully connected Dense layer with 512 neurons\n",
    "x = Dense(512, activation='relu', name=\"dense1\")(x)\n",
    "\n",
    "# Fully connected Dense layer with 10 neurons (output layer for classification)\n",
    "x_out = Dense(10, activation='softmax', name=\"dense2\")(x)\n",
    "\n",
    "# Create the QKeras model\n",
    "new_qmodel = Model(inputs=[x_in], outputs=[x_out], name='qkeras')\n",
    "\n",
    "new_qmodel.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To leverage the capabilities of the MDC tool of merging different CNN models into onereconfigurable accelerator, we take the original model, change the precision of one or more layers, and freeze the others during training while fine-tuning is applied to the changed layers. This enables MDC to reuse the common, unchanged layers. In this example, we changed the precision of the second Convolutional layer (\"q_conv2d_1\") from 4 to 8 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_in = Input(shape=input_shape, name = \"input_layer\")\n",
    "\n",
    "x = QConv2D(32, (3,3), name = \"q_conv2d\", padding='same',kernel_quantizer= quantized_bits(bits=8, integer=4, alpha=1),\n",
    "    bias_quantizer= quantized_bits(bits=8, integer=4, alpha=1))(x)\n",
    "x = QActivation(quantized_relu(bits=16, integer=8, use_sigmoid=0, negative_slope=0.0), name=\"act_1\")(x)\n",
    "x = MaxPooling2D(pool_size=(2,2),name = \"max_pool_1\")(x)\n",
    "x = QConv2D(32, (3,3), name = \"q_conv2d_1\",padding='same',kernel_quantizer= quantized_bits(bits=8, integer=4, alpha=1),\n",
    "    bias_quantizer= quantized_bits(bits=4, integer=2, alpha=1))(x)\n",
    "x = QActivation(quantized_relu(bits=16, integer=8, use_sigmoid=0, negative_slope=0.0), name=\"act_2\")(x)\n",
    "x = MaxPooling2D(pool_size=(2,2), name = \"max_pool_2\")(x)\n",
    "x = Flatten(name = \"flatten\")(x)\n",
    "x = QDense((10),name = \"q_dense\",kernel_quantizer= quantized_bits(bits=8, integer=4, alpha=1),\n",
    "    bias_quantizer= quantized_bits(bits=8, integer=4, alpha=1)) (x)   # num_classes = 10\n",
    "x_out = Activation('sigmoid', name='output_sigmoid')(x)\n",
    "\n",
    "new_qmodel = Model(inputs=[x_in], outputs=[x_out], name='qkeras')\n",
    "\n",
    "new_qmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the weights of the original model and freeze every layer, expect the \"q_conv2d_1\" one. AFter this operation, we can start the training of this new model: only the \"q_conv2d_1\" will be able to change the value of its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_qmodel.load_weights(\"model_weights.h5\")\n",
    "\n",
    "for layer in new_qmodel.layers:\n",
    "    if layer.name != \"q_conv2d_1\":\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "\n",
    "n_epochs = 3\n",
    "if train:\n",
    "    LOSS = tf.keras.losses.CategoricalCrossentropy()\n",
    "    OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=3e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "    new_qmodel.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
    "    ]\n",
    "\n",
    "    start = time.time()\n",
    "    history = new_qmodel.fit(train_data, epochs=n_epochs, validation_data=val_data, callbacks=callbacks, verbose=1)\n",
    "    end = time.time()\n",
    "    print('\\n It took {} minutes to train!\\n'.format((end - start) / 60.0))\n",
    "\n",
    "\n",
    "    new_qmodel.save('model_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check if effecively the new model's weights are changed. After this chechk, we can convert also this new model into the QONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the names of layers with matching and non-matching weights\n",
    "matching_layers = []\n",
    "non_matching_layers = []\n",
    "\n",
    "for original_layer, modified_layer in zip(qmodel.layers, new_qmodel.layers):\n",
    "    \n",
    "\n",
    "    # Check if the layer has weights (some layers like Dropout do not)\n",
    "    if original_layer.get_weights() and modified_layer.get_weights():\n",
    "        # Compare weights\n",
    "        weights_match = all(\n",
    "            (original_weight == modified_weight).all()\n",
    "            for original_weight, modified_weight in zip(original_layer.get_weights(), modified_layer.get_weights())\n",
    "        )\n",
    "        \n",
    "        if weights_match:\n",
    "            matching_layers.append(original_layer.name)\n",
    "        else:\n",
    "            non_matching_layers.append(original_layer.name)\n",
    "\n",
    "print(\"Layers with matching weights:\", matching_layers)\n",
    "print(\"Layers with non-matching weights:\", non_matching_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversion to qonnx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 15:26:40.144136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-22 15:26:40.144267: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2025-01-22 15:26:40.144340: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2025-01-22 15:26:40.144710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-22 15:26:40.144767: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-01-22 15:26:40.246625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-22 15:26:40.246745: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2025-01-22 15:26:40.246835: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2025-01-22 15:26:40.247208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-22 15:26:40.247266: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from qonnx.converters import from_keras\n",
    "\n",
    "path = output_path + '/qonnx_model_2.onnx'\n",
    "print(\"conversion to qonnx...\")\n",
    "qonnx_model, _  = from_keras(\n",
    "    new_qmodel,\n",
    "    name=\"qkeras_to_qonnx_converted\",\n",
    "    input_signature=None,\n",
    "    opset=None,\n",
    "    custom_ops=None,\n",
    "    custom_op_handlers=None,\n",
    "    custom_rewriter=None,\n",
    "    inputs_as_nchw=None,\n",
    "    extra_opset=None,\n",
    "    shape_override=None,\n",
    "    target=None,\n",
    "    large_model=False,\n",
    "    output_path = path,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
